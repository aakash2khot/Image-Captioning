{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import pickle\n","import string\n","import os\n","from numpy import array\n","import glob\n","from pickle import dump, load\n","from time import time\n","from tqdm import tqdm\n","\n","import torch\n","import tensorflow as tf\n","import torchvision.transforms as transforms\n","import keras\n","from keras.applications.inception_v3 import preprocess_input, InceptionV3\n","from keras.preprocessing import sequence, image\n","from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n","from keras.optimizers import Adam, RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Model, Sequential\n","from keras import Input, layers, optimizers\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import random\n","!pip install -U nltk\n","!pip install nltk==3.5 \n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_f = open(\"/kaggle/input/flickr-8k/Flickr8k_text/Flickr8k_text/Flickr_8k.trainImages.txt\")\n","train_image_paths = train_f.read().splitlines()\n","\n","test_f = open(\"/kaggle/input/flickr-8k/Flickr8k_text/Flickr8k_text/Flickr_8k.testImages.txt\")\n","test_image_paths = test_f.read().splitlines()\n","\n","val_f = open(\"/kaggle/input/flickr-8k/Flickr8k_text/Flickr8k_text/Flickr_8k.valImages.txt\")\n","val_image_paths = val_f.read().splitlines()\n","\n","val_image_paths[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cap_f = open(\"/kaggle/input/flickr-8k/Flickr8k_text/Flickr8k_text/Flickr8k.token.txt\")\n","captns = cap_f.read().splitlines()\n","\n","captions = {}\n","\n","for cap in captns:\n","    img, temp = cap.split(\"\\t\")\n","    if img[:-2] in captions.keys():\n","        captions[img[:-2]].append(temp)\n","    else:\n","        captions[img[:-2]] = [temp]\n","\n","# prepare translation table for removing punctuation\n","table = str.maketrans('', '', string.punctuation)\n","\n","for img, caption_list in captions.items():\n","    for i in range(len(caption_list)):\n","        temp = caption_list[i]\n","        temp = temp.split()\n","        temp = [word.lower() for word in temp]\n","        # remove punctuation from each token\n","        temp = [w.translate(table) for w in temp]\n","        # remove hanging 's' and 'a'\n","        temp = [word for word in temp if len(word)>1]\n","        # remove tokens with numbers in them\n","        temp = [word for word in temp if word.isalpha()]\n","        # store as string\n","        caption_list[i] =  ' '.join(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_cap = {}\n","for img in train_image_paths:\n","    train_cap[img] = captions[img]\n","\n","test_cap = {}\n","for img in test_image_paths:\n","    test_cap[img] = captions[img]\n","    \n","val_cap = {}\n","for img in val_image_paths:\n","    val_cap[img] = captions[img]    \n","    \n","val_cap['2090545563_a4e66ec76b.jpg']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for img, caption_list in train_cap.items():\n","    for i in range(len(caption_list)):\n","        caption_list[i] =  'startseq ' + caption_list[i] + ' endseq'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val_cap, test_cap and train_cap are dictionies with keys as image names and values as a list of captions for the corresponding image."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Code for creating vision embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Load the inception v3 model\n","# model = InceptionV3(weights='imagenet')\n","# # Create a new model, by removing the last layer (output layer) from the inception v3\n","# model_new = Model(model.input, model.layers[-2].output)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def encode(img):\n","#     img = Image.open(img)\n","#     img = img.resize((299,299))\n","#     img = np.asarray(img)\n","#     img = np.expand_dims(img, axis=0)\n","#     img = preprocess_input(img)\n","#     img_enc = model_new.predict(img)\n","#     img_enc = np.reshape(img_enc, img_enc.shape[1])\n","#     return img_enc"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"source":["# train_encoding = {}\n","# for img in tqdm(train_image_paths):\n","#     train_encoding[img] = encode(\"/kaggle/input/flickr-8k/Flicker8k_Images/Flicker8k_Images/\" + img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val_encoding = {}\n","# for img in tqdm(val_image_paths):\n","#     val_encoding[img] = encode(\"/kaggle/input/flickr-8k/Flicker8k_Images/Flicker8k_Images/\" + img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_encoding = {}\n","# for img in tqdm(test_image_paths):\n","#     test_encoding[img] = encode(\"/kaggle/input/flickr-8k/Flicker8k_Images/Flicker8k_Images/\" + img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open('/kaggle/working/train_vis_embeddings.pickle', 'wb') as handle:\n","#     pickle.dump(train_encoding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","# with open('/kaggle/working/val_vis_embeddings.pickle', 'wb') as handle:\n","#     pickle.dump(val_encoding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# with open('/kaggle/working/test_vis_embeddings.pickle', 'wb') as handle:\n","#     pickle.dump(test_encoding, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Reading vision embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('/kaggle/input/vr-project-embeddings/train_vis_embeddings.pickle', 'rb') as handle:\n","    train_encoding = pickle.load(handle)\n","\n","with open('/kaggle/input/vr-project-embeddings/val_vis_embeddings.pickle', 'rb') as handle:\n","    val_encoding = pickle.load(handle)\n","\n","with open('/kaggle/input/vr-project-embeddings/test_vis_embeddings.pickle', 'rb') as handle:\n","    test_encoding = pickle.load(handle)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Word embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Vocabulary size of captions\n","\n","vocabulary = set()\n","for key in train_cap.keys():\n","    [vocabulary.update(d.split()) for d in train_cap[key]]\n","print('Train Vocabulary Size: %d' % len(vocabulary))\n","\n","for key in val_cap.keys():\n","    [vocabulary.update(d.split()) for d in val_cap[key]]\n","\n","for key in test_cap.keys():\n","    [vocabulary.update(d.split()) for d in test_cap[key]]\n","print('Total Vocabulary Size: %d' % len(vocabulary))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["words_list = []\n","for key, values in train_cap.items():\n","    for caption in values:\n","        words_list.extend(caption.split(\" \"))\n","\n","word_count_threshold = 10\n","word_count = {}\n","\n","for word in words_list:  \n","    if word in word_count.keys():\n","        word_count[word] += 1\n","    else:\n","        word_count[word] = 1\n","        \n","# modified_vocab has only those words that have occured at least 10 times in all the captions of the train images\n","modified_vocab = [word for word in word_count if word_count[word] >= word_count_threshold]\n","len(modified_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["indexToWord = {}\n","wordToIndex = {}\n","index = 1\n","\n","for w in modified_vocab:\n","    wordToIndex[w] = index\n","    indexToWord[index] = w\n","    index += 1\n","    \n","vocab_size = len(indexToWord) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# The following cell aims to find the maximum number of words in all of the train captions\n","\n","# calculate the length of the description with the most words\n","def max_length(captions):\n","    # convert a dictionary of clean captions to a list of captions\n","    def convertToLines(captions):\n","        complete_desc = list()\n","        for key in captions.keys():\n","            [complete_desc.append(d) for d in captions[key]]\n","        return complete_desc\n","\n","    lines = convertToLines(captions)\n","    return max(len(d.split()) for d in lines)\n","\n","# determine the maximum sequence length\n","max_length = max_length(train_cap)\n","print('Max Description Length: %d' % max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# data generator, intended to be used in a call to model.fit_generator()\n","def dataGenerator(imageCaptions, images, wordToIndex, max_length, num_images_per_batch):\n","    X1, X2, y = list(), list(), list()\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for img, imgCapList in imageCaptions.items():\n","            n+=1\n","            imgEnC = images[img]\n","            for cap in imgCapList:\n","                # encode the sequence\n","                seq = [wordToIndex[word] for word in cap.split(' ') if word in wordToIndex]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    X1.append(imgEnC)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            if n==num_images_per_batch:\n","                yield [[array(X1), array(X2)], array(y)]\n","                X1, X2, y = list(), list(), list()\n","                n=0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load Glove vectors\n","\n","complete_word_embeddings = {}\n","f = open('/kaggle/input/glove6b200d/glove.6B.200d.txt', encoding=\"utf-8\")\n","\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    complete_word_embeddings[word] = coefs\n","    \n","f.close()\n","\n","print('Found %s word vectors.' % len(complete_word_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["word_embedding_dimension = 200\n","req_word_embeddings = np.zeros((vocab_size, word_embedding_dimension))\n","\n","for word, i in wordToIndex.items():\n","    embedding_vector = complete_word_embeddings.get(word)\n","    if embedding_vector is not None:\n","        req_word_embeddings[i] = embedding_vector"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["imgInput = Input(shape=(2048,))\n","imgDropout = Dropout(0.5)(imgInput)\n","imgDense = Dense(256, activation='relu')(imgDropout)\n","wordInput = Input(shape=(max_length,))\n","wordEmb = Embedding(vocab_size, word_embedding_dimension, mask_zero=True)(wordInput)\n","wordDropout = Dropout(0.5)(wordEmb)\n","wordNN = LSTM(256)(wordDropout)\n","combineImgWord = keras.layers.add([imgDense, wordNN])\n","combineDense = Dense(256, activation='relu')(combineImgWord)\n","outputs = Dense(vocab_size, activation='softmax')(combineDense)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Model(inputs=[imgInput, wordInput], outputs=outputs)\n","model.layers[2].set_weights([req_word_embeddings])\n","model.layers[2].trainable = False\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epochs = 20\n","number_imgs_per_batch = 3\n","steps = len(train_cap) // number_imgs_per_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for i in range(epochs):\n","#     generator = dataGenerator(train_cap, train_encoding, wordToIndex, max_length, number_imgs_per_batch)\n","#     model.fit_generator(generator, epochs = 1, steps_per_epoch = steps, verbose = 1)\n","#     model.save('/kaggle/working/model_weights/model_' + str(i) + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Model(inputs=[imgInput, wordInput], outputs=outputs)\n","model.layers[2].set_weights([req_word_embeddings])\n","model.layers[2].trainable = False\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Restore the weights\n","model.load_weights('/kaggle/input/model-weights/model_19.h5')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def greedySearch(img, model, max_length = 34):\n","    pred_caption = 'startseq'\n","    completePred = False\n","    for _ in range(max_length):\n","        intermediate_caption = [wordToIndex[w] for w in pred_caption.split() if w in wordToIndex]\n","        intermediate_caption = tf.keras.preprocessing.sequence.pad_sequences([intermediate_caption], maxlen=max_length)\n","        pred_index = np.argmax(model.predict([img,intermediate_caption], verbose=0))\n","        word = indexToWord[pred_index]\n","        pred_caption += ' ' + word\n","        if word == 'endseq':\n","            completePred = True\n","            break\n","    if completePred:\n","        return ' '.join(pred_caption.split()[1:-1])\n","    return ' '.join(pred_caption.split()[1:])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["imgName = list(train_encoding.keys())[512]\n","imgEncoding = train_encoding[imgName].reshape((1,2048))\n","plt.imshow(plt.imread(\"/kaggle/input/flickr-8k/Flicker8k_Images/Flicker8k_Images/\" + imgName))\n","plt.show()\n","print(\"Greedy search output:\",greedySearch(imgEncoding, model))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_model(image_names, image_embeddings, image_cap, model, seq):\n","    b_scores = 0\n","    m_scores = 0\n","    count = 0\n","    for image_name in tqdm(image_names):    \n","        img_end = image_embeddings[image_name].reshape((1, 2048))\n","        prediction = greedySearch(img_end, model)\n","        \n","        modified_sentence = []\n","        modified_word_tokens = []\n","        \n","        for reference in image_cap[image_name]:\n","            final = reference.split().copy()\n","            if seq:\n","                final = final[1:-1]\n","            modified_word_tokens.append(final)\n","            final = ' '.join(final)\n","            modified_sentence.append(final)\n","        \n","        b_score = sentence_bleu(modified_word_tokens, prediction.split())\n","        m_score = meteor_score(modified_sentence, prediction)        \n","        b_scores += b_score\n","        m_scores += m_score\n","        count += 1\n","\n","    return b_scores/count, m_scores/count"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["b, m = evaluate_model(test_image_paths, test_encoding, test_cap, model, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"BLEU score = \", b)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"METEOR score = \", m)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
